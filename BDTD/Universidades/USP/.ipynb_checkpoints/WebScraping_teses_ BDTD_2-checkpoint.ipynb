{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script para buscar resumos das teses elaboradas pelos empregados da Petrobras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\upe2\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\upe2\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\upe2\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\upe2\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\upe2\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\upe2\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langdetect import detect\n",
    "from langdetect import detect_langs\n",
    "from keras.models import load_model\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo configurações globais de proxy para realizar a extração dentro da rede Petrobras\n",
    "chave = 'upe2'\n",
    "pwd = 'fBO61290'\n",
    "proxy_url = 'http://'+chave+':'+pwd+'@inet-sys.gnet.petrobras.com.br:804/'\n",
    "proxies = {\n",
    "  'http' : proxy_url ,\n",
    "  'https' : proxy_url ,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente entraremos no site da BDTD e buscaremos os links de todas as teses de uma determinada intituição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para coletar os links das tese\n",
    "\n",
    "def get_links(page):\n",
    "        \n",
    "    #preparar a url\n",
    "    url = ('http://bdtd.ibict.br/vufind/Search/Results?filter%5B%5D=institution%3A\"USP\"&type=AllFields&page=' +\n",
    "           str(page))\n",
    "    \n",
    "    #Fazer requisição e parsear o arquivo html\n",
    "    f = requests.get(url).text #, proxies = proxies).text \n",
    "    soup = bs(f, \"html.parser\")\n",
    "\n",
    "    #Coletando link para as teses\n",
    "    links = []\n",
    "    for doc in soup.find_all('a', href=True):\n",
    "        if 'title' in doc.get('class', []):\n",
    "            links.append(doc['href'])\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22000  links capturados,  1100  páginas\n",
      "24000  links capturados,  1200  páginas\n",
      "26000  links capturados,  1300  páginas\n",
      "28000  links capturados,  1400  páginas\n",
      "30000  links capturados,  1500  páginas\n",
      "32000  links capturados,  1600  páginas\n",
      "34000  links capturados,  1700  páginas\n",
      "36000  links capturados,  1800  páginas\n",
      "38000  links capturados,  1900  páginas\n",
      "39980  links capturados,  1999  páginas\n"
     ]
    }
   ],
   "source": [
    "#Coletar o link de todas as teses\n",
    "start_page = 1001\n",
    "n_pages = 2000 # Cada página retorna 20 teses\n",
    "\n",
    "links = []\n",
    "\n",
    "for p in range(start_page, n_pages):\n",
    "    link = get_links(p)\n",
    "    if link != []:\n",
    "        links = links + link\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "    if p % 100 == 0:\n",
    "        print (p*20, ' links capturados, ', p, ' páginas')\n",
    "        with open('links_usp', \"w\") as output:\n",
    "            writer = csv.writer(output, lineterminator='\\n')\n",
    "            for val in links:\n",
    "                writer.writerow([val])\n",
    "                \n",
    "with open('links_usp', \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    for val in links:\n",
    "        writer.writerow([val]) \n",
    "print (p*20, ' links capturados, ', p, ' páginas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrindo arquivo gravado anteriormente\n",
    "\n",
    "#links = []\n",
    "#with open('links_usp', 'r') as f:\n",
    "#    reader = csv.reader(f)\n",
    "#    for link in reader:\n",
    "#        links.append(link[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida vamos recuperar os metadados de cada link coletado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para buscar os metadados das teses no BDTD\n",
    "def tese_link(link):\n",
    "    #definir url\n",
    "    url = 'http://bdtd.ibict.br' + link\n",
    "    \n",
    "    #Requisitar html e fazer o parser\n",
    "    f = requests.get(url).text #, proxies = proxies).text \n",
    "    soup = bs(f, \"html.parser\")\n",
    "\n",
    "    #Dicionário para armazenar as informações da tese\n",
    "    tese = {}  \n",
    "    \n",
    "    #Adicionar título\n",
    "    tese['Title'] = soup.find('h3').get_text()\n",
    "    for doc in soup.find_all('tr'):\n",
    "        #Identificar atributo\n",
    "        try:\n",
    "            atributo = doc.find('th').get_text()\n",
    "        except:\n",
    "            pass\n",
    "        #Verificar se o atributo possui mais de um dado\n",
    "        for row in doc.find_all('td'):\n",
    "            #Adicionar o atributo no dicionário\n",
    "            if row.find('div') == None:\n",
    "                try:\n",
    "                    tese[atributo] = doc.find('td').get_text()\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                element = []\n",
    "                #No dicionário, adicionar todos os dados ao seu respectivo atributo\n",
    "                for e in doc.find_all('div'):\n",
    "                    try:\n",
    "                        sub_e = []\n",
    "                        for sub_element in e.find_all('a'):\n",
    "                            element.append(sub_element.get_text()) \n",
    "                        #element.append(sub_e)\n",
    "                    except:\n",
    "                        pass\n",
    "                tese[atributo] = element\n",
    "    \n",
    "    return(tese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como em alguns casos o resumo português e inglês se misturaram, foi implementado uma função para separar os textos misturados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para separar resumos português e inglês\n",
    "def separacao_port_engl(abstract):\n",
    "\n",
    "    mix_sent = nltk.sent_tokenize(abstract)\n",
    "\n",
    "    new_mix = []\n",
    "    for sent in mix_sent:\n",
    "        position = sent.find('.')\n",
    "        if position != len(sent)-1:\n",
    "            sent_1 = sent[:position+1]\n",
    "            sent_2 = sent[position+1:]\n",
    "            new_mix.append(sent_1)\n",
    "            new_mix.append(sent_2)\n",
    "        else:\n",
    "            new_mix.append(sent)\n",
    "\n",
    "    mix_sent = new_mix\n",
    "\n",
    "    port = []\n",
    "    engl = []\n",
    "\n",
    "    for sent in mix_sent:\n",
    "        try:\n",
    "            if detect (sent) == 'pt':\n",
    "                port.append(sent)\n",
    "            else:\n",
    "                engl.append (sent)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    port = \" \".join(port)\n",
    "    engl = \" \".join(engl)\n",
    "\n",
    "    return(port, engl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até esse momento estamos recuperando as informações de todas as teses de uma determinada instituição. No entanto o objetivo é gravar os metadados e salvar o arquivo apenas das teses relacionadas a O&G. Portanto, vamos carregar os algoritmos de classificação e de vetorização de palavras treinados previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 400, 50)           9289150   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_6 (Spatial (None, 400, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 396, 128)          32128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 198, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 198, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 194, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 97, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 97, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 93, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 9,814,591\n",
      "Trainable params: 525,441\n",
      "Non-trainable params: 9,289,150\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Carregando modelo Word2Vec\n",
    "BDTD_word2vec_50 = Word2Vec.load(\"..\\..\\..\\Embeddings\\BDTD_word2vec_50\")\n",
    "# Carregando modelo keras\n",
    "model_keras = load_model('..\\..\\..\\model_cnn.h5')\n",
    "model_keras.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicionário proveniente do modelo de word embedding para converter palavras em índices\n",
    "word2index = {}\n",
    "for index, word in enumerate(BDTD_word2vec_50.wv.index2word):\n",
    "    word2index[word] = index\n",
    "    \n",
    "# Função para converter texto em sequência de índices\n",
    "def index_pad_text(text, maxlen, word2index):\n",
    "    maxlen = 400\n",
    "    new_text = [] \n",
    "    \n",
    "    for word in word_tokenize(text):\n",
    "        try:\n",
    "            new_text.append(word2index[word])\n",
    "        except:\n",
    "            pass\n",
    "    # Add the padding for each sentence. Here I am padding with 0\n",
    "    if len(new_text) > maxlen:\n",
    "        new_text = new_text[:400]\n",
    "    else:\n",
    "        new_text += [0] * (maxlen - len(new_text))\n",
    "\n",
    "    return np.array(new_text)\n",
    "\n",
    "maxlen = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada link coletado será feita as seguintes tarefas:\n",
    "* verificar se o texto português e inglês estão misturados;\n",
    "* transformar o texto em sequência de índices;\n",
    "* classificar quanto a relevância ao domínio de O&G;\n",
    "* se for relevante, gravar os metadados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46  teses avaliadas e  1  teses relacionadas a O&G encontradas.\n",
      "426  teses avaliadas e  2  teses relacionadas a O&G encontradas.\n",
      "444  teses avaliadas e  3  teses relacionadas a O&G encontradas.\n",
      "513  teses avaliadas e  4  teses relacionadas a O&G encontradas.\n",
      "540  teses avaliadas e  5  teses relacionadas a O&G encontradas.\n",
      "726  teses avaliadas e  6  teses relacionadas a O&G encontradas.\n",
      "736  teses avaliadas e  7  teses relacionadas a O&G encontradas.\n",
      "747  teses avaliadas e  8  teses relacionadas a O&G encontradas.\n",
      "952  teses avaliadas e  9  teses relacionadas a O&G encontradas.\n",
      "1071  teses avaliadas e  10  teses relacionadas a O&G encontradas.\n",
      "1137  teses avaliadas e  11  teses relacionadas a O&G encontradas.\n",
      "1158  teses avaliadas e  12  teses relacionadas a O&G encontradas.\n",
      "1161  teses avaliadas e  13  teses relacionadas a O&G encontradas.\n",
      "1330  teses avaliadas e  14  teses relacionadas a O&G encontradas.\n",
      "1458  teses avaliadas e  15  teses relacionadas a O&G encontradas.\n",
      "1485  teses avaliadas e  16  teses relacionadas a O&G encontradas.\n",
      "1516  teses avaliadas e  17  teses relacionadas a O&G encontradas.\n",
      "1563  teses avaliadas e  18  teses relacionadas a O&G encontradas.\n",
      "1571  teses avaliadas e  19  teses relacionadas a O&G encontradas.\n",
      "1886  teses avaliadas e  20  teses relacionadas a O&G encontradas.\n",
      "1904  teses avaliadas e  21  teses relacionadas a O&G encontradas.\n",
      "2002  teses avaliadas e  22  teses relacionadas a O&G encontradas.\n",
      "2012  teses avaliadas e  23  teses relacionadas a O&G encontradas.\n",
      "2073  teses avaliadas e  24  teses relacionadas a O&G encontradas.\n",
      "2140  teses avaliadas e  25  teses relacionadas a O&G encontradas.\n",
      "2417  teses avaliadas e  26  teses relacionadas a O&G encontradas.\n",
      "2448  teses avaliadas e  27  teses relacionadas a O&G encontradas.\n",
      "2483  teses avaliadas e  28  teses relacionadas a O&G encontradas.\n",
      "2723  teses avaliadas e  29  teses relacionadas a O&G encontradas.\n",
      "2735  teses avaliadas e  30  teses relacionadas a O&G encontradas.\n",
      "2817  teses avaliadas e  31  teses relacionadas a O&G encontradas.\n",
      "2918  teses avaliadas e  32  teses relacionadas a O&G encontradas.\n",
      "2929  teses avaliadas e  33  teses relacionadas a O&G encontradas.\n",
      "3070  teses avaliadas e  34  teses relacionadas a O&G encontradas.\n",
      "3088  teses avaliadas e  35  teses relacionadas a O&G encontradas.\n",
      "3537  teses avaliadas e  36  teses relacionadas a O&G encontradas.\n",
      "3781  teses avaliadas e  37  teses relacionadas a O&G encontradas.\n",
      "3924  teses avaliadas e  38  teses relacionadas a O&G encontradas.\n",
      "4189  teses avaliadas e  39  teses relacionadas a O&G encontradas.\n",
      "4203  teses avaliadas e  40  teses relacionadas a O&G encontradas.\n",
      "4387  teses avaliadas e  41  teses relacionadas a O&G encontradas.\n",
      "4564  teses avaliadas e  42  teses relacionadas a O&G encontradas.\n",
      "4802  teses avaliadas e  43  teses relacionadas a O&G encontradas.\n",
      "5263  teses avaliadas e  44  teses relacionadas a O&G encontradas.\n",
      "5290  teses avaliadas e  45  teses relacionadas a O&G encontradas.\n",
      "5331  teses avaliadas e  46  teses relacionadas a O&G encontradas.\n",
      "5549  teses avaliadas e  47  teses relacionadas a O&G encontradas.\n",
      "5734  teses avaliadas e  48  teses relacionadas a O&G encontradas.\n",
      "5837  teses avaliadas e  49  teses relacionadas a O&G encontradas.\n",
      "5904  teses avaliadas e  50  teses relacionadas a O&G encontradas.\n",
      "5973  teses avaliadas e  51  teses relacionadas a O&G encontradas.\n",
      "5977  teses avaliadas e  52  teses relacionadas a O&G encontradas.\n",
      "6206  teses avaliadas e  53  teses relacionadas a O&G encontradas.\n",
      "6864  teses avaliadas e  54  teses relacionadas a O&G encontradas.\n",
      "6873  teses avaliadas e  55  teses relacionadas a O&G encontradas.\n",
      "7350  teses avaliadas e  56  teses relacionadas a O&G encontradas.\n",
      "7516  teses avaliadas e  57  teses relacionadas a O&G encontradas.\n",
      "7537  teses avaliadas e  58  teses relacionadas a O&G encontradas.\n",
      "7559  teses avaliadas e  59  teses relacionadas a O&G encontradas.\n",
      "7561  teses avaliadas e  60  teses relacionadas a O&G encontradas.\n",
      "7574  teses avaliadas e  61  teses relacionadas a O&G encontradas.\n",
      "7594  teses avaliadas e  62  teses relacionadas a O&G encontradas.\n",
      "7656  teses avaliadas e  63  teses relacionadas a O&G encontradas.\n",
      "7670  teses avaliadas e  64  teses relacionadas a O&G encontradas.\n",
      "7676  teses avaliadas e  65  teses relacionadas a O&G encontradas.\n",
      "7690  teses avaliadas e  66  teses relacionadas a O&G encontradas.\n",
      "7733  teses avaliadas e  67  teses relacionadas a O&G encontradas.\n",
      "8053  teses avaliadas e  68  teses relacionadas a O&G encontradas.\n",
      "8118  teses avaliadas e  69  teses relacionadas a O&G encontradas.\n",
      "8251  teses avaliadas e  70  teses relacionadas a O&G encontradas.\n",
      "8255  teses avaliadas e  71  teses relacionadas a O&G encontradas.\n",
      "8310  teses avaliadas e  72  teses relacionadas a O&G encontradas.\n",
      "8387  teses avaliadas e  73  teses relacionadas a O&G encontradas.\n",
      "8392  teses avaliadas e  74  teses relacionadas a O&G encontradas.\n",
      "8398  teses avaliadas e  75  teses relacionadas a O&G encontradas.\n",
      "8426  teses avaliadas e  76  teses relacionadas a O&G encontradas.\n",
      "8427  teses avaliadas e  77  teses relacionadas a O&G encontradas.\n",
      "8736  teses avaliadas e  78  teses relacionadas a O&G encontradas.\n",
      "8741  teses avaliadas e  79  teses relacionadas a O&G encontradas.\n",
      "8756  teses avaliadas e  80  teses relacionadas a O&G encontradas.\n",
      "8758  teses avaliadas e  81  teses relacionadas a O&G encontradas.\n",
      "8762  teses avaliadas e  82  teses relacionadas a O&G encontradas.\n",
      "8765  teses avaliadas e  83  teses relacionadas a O&G encontradas.\n",
      "8772  teses avaliadas e  84  teses relacionadas a O&G encontradas.\n",
      "8774  teses avaliadas e  85  teses relacionadas a O&G encontradas.\n",
      "8790  teses avaliadas e  86  teses relacionadas a O&G encontradas.\n",
      "8811  teses avaliadas e  87  teses relacionadas a O&G encontradas.\n",
      "8813  teses avaliadas e  88  teses relacionadas a O&G encontradas.\n",
      "8888  teses avaliadas e  89  teses relacionadas a O&G encontradas.\n",
      "8929  teses avaliadas e  90  teses relacionadas a O&G encontradas.\n",
      "8977  teses avaliadas e  91  teses relacionadas a O&G encontradas.\n",
      "8984  teses avaliadas e  92  teses relacionadas a O&G encontradas.\n",
      "8985  teses avaliadas e  93  teses relacionadas a O&G encontradas.\n",
      "8990  teses avaliadas e  94  teses relacionadas a O&G encontradas.\n",
      "9003  teses avaliadas e  95  teses relacionadas a O&G encontradas.\n",
      "9008  teses avaliadas e  96  teses relacionadas a O&G encontradas.\n",
      "9013  teses avaliadas e  97  teses relacionadas a O&G encontradas.\n",
      "9022  teses avaliadas e  98  teses relacionadas a O&G encontradas.\n",
      "9026  teses avaliadas e  99  teses relacionadas a O&G encontradas.\n",
      "9032  teses avaliadas e  100  teses relacionadas a O&G encontradas.\n",
      "9046  teses avaliadas e  101  teses relacionadas a O&G encontradas.\n",
      "9234  teses avaliadas e  102  teses relacionadas a O&G encontradas.\n",
      "9322  teses avaliadas e  103  teses relacionadas a O&G encontradas.\n",
      "9460  teses avaliadas e  104  teses relacionadas a O&G encontradas.\n",
      "9611  teses avaliadas e  105  teses relacionadas a O&G encontradas.\n",
      "9670  teses avaliadas e  106  teses relacionadas a O&G encontradas.\n",
      "9681  teses avaliadas e  107  teses relacionadas a O&G encontradas.\n",
      "9711  teses avaliadas e  108  teses relacionadas a O&G encontradas.\n",
      "9772  teses avaliadas e  109  teses relacionadas a O&G encontradas.\n",
      "9780  teses avaliadas e  110  teses relacionadas a O&G encontradas.\n",
      "9805  teses avaliadas e  111  teses relacionadas a O&G encontradas.\n",
      "9813  teses avaliadas e  112  teses relacionadas a O&G encontradas.\n",
      "9817  teses avaliadas e  113  teses relacionadas a O&G encontradas.\n",
      "9825  teses avaliadas e  114  teses relacionadas a O&G encontradas.\n",
      "9903  teses avaliadas e  115  teses relacionadas a O&G encontradas.\n",
      "9915  teses avaliadas e  116  teses relacionadas a O&G encontradas.\n",
      "9918  teses avaliadas e  117  teses relacionadas a O&G encontradas.\n",
      "9923  teses avaliadas e  118  teses relacionadas a O&G encontradas.\n",
      "9927  teses avaliadas e  119  teses relacionadas a O&G encontradas.\n",
      "9995  teses avaliadas e  120  teses relacionadas a O&G encontradas.\n",
      "10003  teses avaliadas e  121  teses relacionadas a O&G encontradas.\n",
      "10019  teses avaliadas e  122  teses relacionadas a O&G encontradas.\n",
      "10029  teses avaliadas e  123  teses relacionadas a O&G encontradas.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10164  teses avaliadas e  124  teses relacionadas a O&G encontradas.\n",
      "10651  teses avaliadas e  125  teses relacionadas a O&G encontradas.\n",
      "10671  teses avaliadas e  126  teses relacionadas a O&G encontradas.\n",
      "10696  teses avaliadas e  127  teses relacionadas a O&G encontradas.\n",
      "10757  teses avaliadas e  128  teses relacionadas a O&G encontradas.\n",
      "10994  teses avaliadas e  129  teses relacionadas a O&G encontradas.\n",
      "11249  teses avaliadas e  130  teses relacionadas a O&G encontradas.\n",
      "11320  teses avaliadas e  131  teses relacionadas a O&G encontradas.\n",
      "11506  teses avaliadas e  132  teses relacionadas a O&G encontradas.\n",
      "11539  teses avaliadas e  133  teses relacionadas a O&G encontradas.\n",
      "11545  teses avaliadas e  134  teses relacionadas a O&G encontradas.\n",
      "11623  teses avaliadas e  135  teses relacionadas a O&G encontradas.\n",
      "11746  teses avaliadas e  136  teses relacionadas a O&G encontradas.\n",
      "11800  teses avaliadas e  137  teses relacionadas a O&G encontradas.\n",
      "11821  teses avaliadas e  138  teses relacionadas a O&G encontradas.\n",
      "11851  teses avaliadas e  139  teses relacionadas a O&G encontradas.\n",
      "12044  teses avaliadas e  140  teses relacionadas a O&G encontradas.\n",
      "12150  teses avaliadas e  141  teses relacionadas a O&G encontradas.\n",
      "12188  teses avaliadas e  142  teses relacionadas a O&G encontradas.\n",
      "12190  teses avaliadas e  143  teses relacionadas a O&G encontradas.\n",
      "12192  teses avaliadas e  144  teses relacionadas a O&G encontradas.\n",
      "12198  teses avaliadas e  145  teses relacionadas a O&G encontradas.\n",
      "12353  teses avaliadas e  146  teses relacionadas a O&G encontradas.\n",
      "12401  teses avaliadas e  147  teses relacionadas a O&G encontradas.\n",
      "12435  teses avaliadas e  148  teses relacionadas a O&G encontradas.\n",
      "12617  teses avaliadas e  149  teses relacionadas a O&G encontradas.\n",
      "12832  teses avaliadas e  150  teses relacionadas a O&G encontradas.\n",
      "12941  teses avaliadas e  151  teses relacionadas a O&G encontradas.\n",
      "12963  teses avaliadas e  152  teses relacionadas a O&G encontradas.\n",
      "12994  teses avaliadas e  153  teses relacionadas a O&G encontradas.\n",
      "13018  teses avaliadas e  154  teses relacionadas a O&G encontradas.\n",
      "13453  teses avaliadas e  155  teses relacionadas a O&G encontradas.\n",
      "13574  teses avaliadas e  156  teses relacionadas a O&G encontradas.\n",
      "13576  teses avaliadas e  157  teses relacionadas a O&G encontradas.\n",
      "13587  teses avaliadas e  158  teses relacionadas a O&G encontradas.\n",
      "13786  teses avaliadas e  159  teses relacionadas a O&G encontradas.\n",
      "13860  teses avaliadas e  160  teses relacionadas a O&G encontradas.\n",
      "13871  teses avaliadas e  161  teses relacionadas a O&G encontradas.\n",
      "13938  teses avaliadas e  162  teses relacionadas a O&G encontradas.\n",
      "13977  teses avaliadas e  163  teses relacionadas a O&G encontradas.\n",
      "14195  teses avaliadas e  164  teses relacionadas a O&G encontradas.\n",
      "14207  teses avaliadas e  165  teses relacionadas a O&G encontradas.\n",
      "14292  teses avaliadas e  166  teses relacionadas a O&G encontradas.\n",
      "14298  teses avaliadas e  167  teses relacionadas a O&G encontradas.\n",
      "14323  teses avaliadas e  168  teses relacionadas a O&G encontradas.\n",
      "14477  teses avaliadas e  169  teses relacionadas a O&G encontradas.\n",
      "14568  teses avaliadas e  170  teses relacionadas a O&G encontradas.\n",
      "14646  teses avaliadas e  171  teses relacionadas a O&G encontradas.\n",
      "14746  teses avaliadas e  172  teses relacionadas a O&G encontradas.\n"
     ]
    }
   ],
   "source": [
    "# Dicionário para agrupar os metadados\n",
    "metadados = {}\n",
    "# Contadores te links testados e classificados como O&G\n",
    "n_test = 0\n",
    "n_pet = 0\n",
    "# Testando cada link de links\n",
    "for link in links[14747:]:\n",
    "    n_test += 1\n",
    "    try:\n",
    "        # Recuperar o metadados de uma tese\n",
    "        metadado = tese_link(link)\n",
    "        # Verificar se existe resumo em inglês, separar texto português/inglês e realocar \n",
    "        # os textos separados nas respectivas colunas\n",
    "        if 'Resumo inglês:' not in metadado:\n",
    "            metadado['Resumo inglês:'] = separacao_port_engl(metadado['Resumo Português:'])[1]\n",
    "        metadado['Resumo Português:'] = separacao_port_engl(metadado['Resumo Português:'])[0]\n",
    "        # Colocando o texto em minúscula\n",
    "        text = metadado['Resumo Português:'].lower()\n",
    "        # Convertendo as palavras em sequencias de acordo com o modelo word2vec\n",
    "        text_seq = index_pad_text(text, maxlen, word2index)\n",
    "        text_seq = text_seq.reshape((1, 400))\n",
    "        # Usando o algoritmo classificador para prever se a tese é relevante\n",
    "        pred = model_keras.predict(text_seq)[0]\n",
    "        #Se a classificação for menor do que 0.2 manter os metadados\n",
    "        if (pred < 0.2 and len(text) > 100):\n",
    "            metadado['Classificador'] = pred[0]\n",
    "            texto_completo = metadado['Download Texto Completo:']\n",
    "            metadados[texto_completo] = metadado\n",
    "            n_pet += 1\n",
    "            # Gravando os resultados em JSON\n",
    "            metadados_usp = pd.DataFrame.from_dict(metadados, orient='index')\n",
    "            metadados_usp.to_json('metadados_usp_2_2.json', orient = 'index')\n",
    "            print(n_test, \" teses avaliadas e \", n_pet, \" teses relacionadas a O&G encontradas.\")\n",
    "    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incluindo um ID para cada tese\n",
    "universidade = 'USP'\n",
    "metadados_usp['PDF_ID'] = metadados_usp['Download Texto Completo:'].apply(lambda x: universidade + '_' + x[-20:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadados_usp.to_json('metadados_usp_2_2.json', orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando arquivos já gravados\n",
    "#metadados_usp = pd.read_json('metadados_usp_2.json', orient = 'index')\n",
    "metadados_usp = pd.read_json('metadados_usp_2_2.json', orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadados_usp = metadados_usp[47:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A próxima etapa será fazer o download das teses classificadas como relevante para o domínio de O&G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USP_tde-12122018-074029\n",
      "USP_tde-16072018-083539\n",
      "USP_tde-20042017-091600\n",
      "USP_tde-22032019-111757\n",
      "USP_tde-12072007-191834\n",
      "USP_tde-15052007-170034\n",
      "USP_tde-17012008-110514\n",
      "USP_tde-18082010-162014\n",
      "USP_tde-19042007-164002\n",
      "USP_tde-22052018-133523\n",
      "USP_tde-12092008-153336\n",
      "USP_tde-15122006-104021\n",
      "USP_tde-18122009-125406\n",
      "USP_tde-05032018-161252\n",
      "USP_tde-05092017-103140\n",
      "USP_tde-06072014-192156\n",
      "USP_tde-06082001-150353\n",
      "USP_tde-07012008-175208\n",
      "USP_tde-07062013-112915\n",
      "USP_tde-07082007-163415\n",
      "USP_tde-07112008-140442\n",
      "USP_tde-08092010-123851\n",
      "USP_tde-10052012-122241\n",
      "USP_tde-10062013-152743\n",
      "USP_tde-12122016-145021\n",
      "USP_tde-16072018-092748\n",
      "USP_tde-16072018-112316\n",
      "USP_tde-16102014-151805\n",
      "USP_tde-16122004-090825\n",
      "USP_tde-18082009-142836\n",
      "USP_tde-19012011-153954\n",
      "USP_tde-20062017-090942\n",
      "USP_tde-20062017-132713\n",
      "USP_tde-20102010-170921\n",
      "USP_tde-24092018-103905\n",
      "USP_tde-24102017-112603\n",
      "USP_tde-24122014-115533\n",
      "USP_tde-26122014-165542\n",
      "USP_tde-27122013-110700\n",
      "USP_tde-28122015-164150\n",
      "USP_tde-30072013-220253\n",
      "USP_tde-31052011-164903\n",
      "USP_tde-17072018-073646\n",
      "USP_tde-16072013-171723\n",
      "USP_tde-21022017-093145\n",
      "USP_tde-15092009-161203\n",
      "USP_tde-18122009-141203\n",
      "USP_tde-18092018-090636\n",
      "USP_tde-18112009-094618\n",
      "USP_tde-16072018-091052\n",
      "USP_tde-18112010-123534\n",
      "USP_tde-19032015-171034\n",
      "USP_tde-22042007-132558\n",
      "USP_tde-16062016-143355\n",
      "USP_tde-16122014-161035\n",
      "USP_tde-18072011-102219\n",
      "USP_tde-19092014-102819\n",
      "USP_tde-19092018-093832\n",
      "USP_tde-22062016-084354\n",
      "USP_tde-21012013-093552\n",
      "USP_tde-12122013-155734\n",
      "USP_tde-18072012-102615\n",
      "USP_tde-12072013-152710\n",
      "USP_tde-18072012-094211\n",
      "USP_tde-22062016-101100\n",
      "USP_tde-22062015-145002\n",
      "USP_tde-04052016-094050\n",
      "USP_tde-08012011-201628\n",
      "USP_tde-08092009-162406\n",
      "USP_tde-11112008-103807\n",
      "USP_tde-16032007-163331\n",
      "USP_tde-18102007-103858\n",
      "USP_tde-18122007-115436\n",
      "USP_tde-18122007-145313\n",
      "USP_tde-11012016-152817\n",
      "USP_tde-14112013-111133\n",
      "USP_tde-17062016-101836\n",
      "USP_tde-01072014-125659\n",
      "USP_tde-15082012-104104\n",
      "USP_tde-01032011-110406\n",
      "USP_tde-02032011-161312\n",
      "USP_tde-17062009-142456\n",
      "USP_tde-21032014-120013\n",
      "USP_tde-10032015-165044\n",
      "USP_tde-16012017-162912\n",
      "USP_tde-17042013-104302\n",
      "USP_tde-19032018-144646\n",
      "USP_tde-20122012-100600\n",
      "USP_tde-16052007-001538\n",
      "USP_tde-21012016-142904\n",
      "USP_tde-22062011-100402\n",
      "USP_tde-04042016-103128\n",
      "USP_tde-07052015-094155\n",
      "USP_tde-09092015-151027\n",
      "USP_tde-08052017-154809\n",
      "USP_tde-08062015-170145\n",
      "USP_tde-09012015-095044\n",
      "USP_tde-25052018-094308\n",
      "USP_tde-13092012-105919\n",
      "USP_tde-22082012-121853\n",
      "USP_tde-17032012-224928\n",
      "USP_tde-13082013-091628\n",
      "USP_tde-19122011-175437\n",
      "USP_tde-12082010-154225\n",
      "USP_tde-12082010-213202\n",
      "USP_tde-15062012-094830\n",
      "USP_tde-15072005-144325\n",
      "USP_tde-15072007-114005\n",
      "USP_tde-18072007-155710\n",
      "USP_tde-19072012-104641\n",
      "USP_tde-20052013-115947\n",
      "USP_tde-20082010-200639\n",
      "USP_tde-20122005-115553\n",
      "USP_tde-22072012-125806\n",
      "USP_tde-15092016-142341\n",
      "USP_tde-21012011-094354\n",
      "USP_tde-22042015-163608\n",
      "USP_tde-16092014-171804\n",
      "USP_tde-13082018-145437\n",
      "USP_tde-19052008-140423\n",
      "USP_tde-26082013-161814\n",
      "USP_tde-22082013-165145\n",
      "USP_tde-03102012-121140\n",
      "USP_tde-29032017-093228\n",
      "USP_tde-22082013-102757\n"
     ]
    }
   ],
   "source": [
    "for tese in metadados_usp.iterrows():\n",
    "    print(tese[1]['PDF_ID'])\n",
    "    try:\n",
    "        #preparar a url\n",
    "        url = tese[1]['Download Texto Completo:']\n",
    "\n",
    "        #Fazer requisição e parsear o arquivo html\n",
    "        f = requests.get(url).text #, proxies = proxies).text \n",
    "        soup = bs(f, \"html.parser\")\n",
    "\n",
    "        #Coletando link para arquivo das teses\n",
    "        links = []\n",
    "        for doc in soup.find_all('a', href=True):\n",
    "            if doc['href'][-4:] == '.pdf':\n",
    "                links.append(doc['href'])\n",
    "\n",
    "        #Recuperando e gravando arquivo PDF\n",
    "        url = 'http://www.teses.usp.br' + links[0]\n",
    "        pdf = requests.get(url) #, proxies = proxies)\n",
    "        filename = tese[1]['PDF_ID'] + '.pdf'\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(pdf.content)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
